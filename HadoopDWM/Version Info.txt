v1.01
    - Processes from Tokenization to ER Matrix 
    - Added 'parmStage' file to capture a copy of each parameter file for the jobs
    - Removed the request for dataset file and parameter file to only parameter file 
    - Made correction to CCMR chaining of records that are in descending order
    - Added 'pair-self' to SMCR (linked pairs) output to capture each pair linked to itself
    - Eliminated COMR script since its purpose has been tackled in te 'pair-self' change
    - Corrected the sorting of EEPR and EFCR outputs to handle numerics

v1.02
    - Works, achieves same Results as DWM until TC Iterations
    - Added an exit condition to driver file (HDWM00_Driver.sh) to only execute job if
      the specified parameter file exists in the present working directory
    - Fixed undercounting bug in TLR by printing the last record
    - Fixed overcounting bug in BTMR by removing pairs formed for last record
    - Fixed bug in BTMR's formation of block-by-pairs blocking keys
    - Fixed bug in TM where same index was given to duplicate tokens in a reference

v1.03
    - Edit the Driver file to read from any current directory using $(pwd) in bash
        - Before executing the driver file, cd to where the program is stored and run the driver from there
        - Due to this change, the 'check.txt' file was changed in HDWM055_CCMRR
    - Created a Log_File using the current time and data
        - used "export" to make logfile available for py files using os.environs
    - Edited python scripts to report to Log_File
    
v1.04 
    - Changed the file "check.txt" to "reportTCiteration.txt"
    - Modified LKIM and LKIR scripts to fit as the input for Cluster Evaluation
    - Added Cluster Evaluation script (HDWM070_CECR.py)
    - Modified the ER Matrix calculator to combine all calculations in one reducer
    - Modified Blocking by adding the steps of TLR to BTPM to help in program iteration 
    - Created Program Loop (starts from Blocking, ends at Cluster Eval)
    - Added a HDWM075 mapper and reducer file to tag all refs that have been used in previous Iterations
    - Fixed wrongful mapping bug in LKIM and LKIR scripts
    - Extended program loop to cover LKIM and LKIR

v1.05
    - Corrected error in HDWM070_CECR Entropy calculator (from "if quality>=entropy" to "if quality>=epsilon")
    - Corrected error in HDWM070_CECR by correctly zipping Bad Clusters to the correct refIDs and Tokens
    - Modified HDWM060 mapper and reducer to correctly identify used refs in GoodClusters and only select unused and Bad refs for reprocessing

v1.06
    - Modified Distributed Cache by letting HDFS create a symlink for parms file which is then read in all scripts
    - Corrected tokenization error in TM script (especially for S6GeCo dataset)
    - Changed all program loop log files to "tmpReport.txt"
    - Renamed all outputs to an explanatory ones
    - Merged Tokenization and Frequency generation processes as one job instead of two seperate jobs 

v1.07    
    - Introduced Cluster Profile Process (HDWM080_CPM, HDWM080_CPR, HDWM080_CPRR)

v1.08 (Beginning of Parallelism)
    - Fixed mapred.framework.name error in mapred-site.xml file. Changed it to mapreduce.framework.name. 
        - This fixed the problem of MR running locally and now runs multiple mappers in Parallelism
    - Fixed inability to locate 'textdistance' library by Hadoop compute nodes
        - Appended location of textdistance to SMCR scrip < sys.path.append(textdistance download Dir) >
    - Created a tmpDir in /usr/local/jobTmp to store job statistics locally

v1.09
    - Added "Average Records per Cluster" statistic to HDWM080_CPRR
    - Discontinued "hasHeader" parameter due to removal of 1st row if set to True. Deletes more than header row 
    - Added sed command to HDWM00_Driver file to remove header from input data before sending to HDFS
    - Fixed skipping header in HDWM080_CPM in distributed environment

v1.10
    - Change header skip to when reading the parms file in from the driver
    - Local logging improved by creating a temporary directory in pwd
    - Added D,FP,FN,and TN statistics to HDWM099_ERMR (ER Matrix)

v1.11
    - Made the JobLog directory a central dir to store all paths & logs for Hadoop jobs
    - Added bash commands to Driver to extract useful statistics from yarn counters after each job (New way of reporting)
    - Created Custom TC Counters using sys.stderr in script to report total merge-state back to driver file
    - Created custom counters in Blocking, Linking, Cluster Eval, etc. for logging and decision making
    - Fixed local worker node logging to their local directory rather than to master directory when using multiple nodes
    - Eliminated logfile,path, and path2, found in all scripts
        - This solution solves error occuring when running (opening files) in multiple nodes 

v1.12
    - More efficient way of reading the Parms file
    - Ability to import DWM modules stored in a zip file

v1.13
    - Condition in DriverFile to only run ER Matrix if truthFile is available

v1.14
    - Remaned Modules and Converted Scripts as Modules whose Functions can be imported 

v1.15
    - Remaned parms to parmsStage.txt for consistency between SDWM and HDWM 

v1.16
    - Added "sysCheck.sh" to extract system information before job starts

v1.17
    - Renamed HDWM to HadoopDWM

v1.18
    - Unused libraries cleanup

v1.19
    - New way to append textdistance library location

v1.20
    - Introduced Driver.sh to automatically execute runHadoopDWM
    - Driver script also extracts systems requirements and updates all needed files